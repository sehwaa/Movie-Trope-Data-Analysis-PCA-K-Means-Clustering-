{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SEHWA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SEHWA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib tk\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트룹 읽어오기\n",
    "path_dir = 'E:/공모전/TropeFlim'\n",
    "file_list = os.listdir(path_dir)\n",
    "\n",
    "mydoclist = []\n",
    "\n",
    "for item in file_list:\n",
    "    mydoclist.append(item)\n",
    "    \n",
    "trope_list = []\n",
    "\n",
    "for item in mydoclist:\n",
    "    f = open(\"E:/공모전/Trope/\" + item , 'r')\n",
    "    temp = \"\"\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        line = line.replace('\\n', '')\n",
    "        if not line : break\n",
    "        else : \n",
    "            temp += line + \" \"\n",
    "    trope_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 영화의 트룹들 토큰화 & 불용어 제거를 위해 소문자화\n",
    "movie_tropes = []\n",
    "for i in range(0, len(trope_list)):\n",
    "    trope_word = trope_list[i].lower().split(' ')\n",
    "    movie_tropes.append(trope_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트(영어) 전처리(Lemmatization 음소표기법)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = []\n",
    "for i in range(0, len(movie_tropes)):\n",
    "    words.append([wordnet_lemmatizer.lemmatize(w) for w in movie_tropes[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거\n",
    "for i in range(0, len(words)):\n",
    "    words[i] = [w for w in words[i] if not w in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰화된 단어들 붙이기\n",
    "movie_list = []\n",
    "for i in range(0, len(words)):\n",
    "    trope = \"\"\n",
    "    for j in range(0, len(words[i])):\n",
    "        trope += words[i][j] + \" \"\n",
    "    movie_list.append(trope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Matrix 생성\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(movie_list)\n",
    "\n",
    "#numpy array로 변환 (sparse matrix 이므로 todense() 함수 호출)\n",
    "X = np.array(tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA Scree Plot으로 주성분 수 탐색(임의로 주성분 6개로 탐색)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=6)\n",
    "pca.fit(X)\n",
    "\n",
    "Z = pca.transform(X)\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#주성분 수 2개로 PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "Z = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means Clustering Scree Plot으로 클러스터 수 탐색\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "distortions = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(Z)\n",
    "    kmeanModel.fit(Z)\n",
    "    distortions.append(sum(np.min(cdist(Z, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / Z.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=4, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#K-means Clustering\n",
    "num_clusters = 4\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시각화\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(Z)\n",
    "results['category'] = km.labels_\n",
    "results\n",
    "\n",
    "colormap = { 0: 'red', 1: 'green', 2: 'blue', 3: 'purple'}\n",
    "colors = results.apply(lambda row: colormap[row.category], axis=1)\n",
    "ax = results.plot(kind='scatter', x=0, y=1, alpha=0.1, s=300, c=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불필요한 트룹 뽑아내기\n",
    "read_tfidf_matrix = pd.read_csv('tfidf_matrix.csv', engine='python')\n",
    "\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "pca.fit(read_tfidf_matrix)\n",
    "\n",
    "xvector = pca.components_[0] # see 'prcomp(my_data)$rotation' in R\n",
    "yvector = pca.components_[1]\n",
    "\n",
    "xs = pca.transform(read_tfidf_matrix)[:,0] # see 'prcomp(my_data)$x' in R\n",
    "ys = pca.transform(read_tfidf_matrix)[:,1]\n",
    "\n",
    "\n",
    "\n",
    "#biplot 그리기\n",
    "for i in range(len(xvector)):\n",
    "# arrows project features (ie columns from csv) as vectors onto PC axes\n",
    "    plt.arrow(0, 0, xvector[i]*max(xs), yvector[i]*max(ys),\n",
    "              color='r', width=0.0005, head_width=0.0025)\n",
    "    plt.text(xvector[i]*max(xs)*1.2, yvector[i]*max(ys)*1.2,\n",
    "             list(read_tfidf_matrix.columns.values)[i], color='r')\n",
    "\n",
    "for i in range(len(xs)):\n",
    "# circles project documents (ie rows from csv) as points onto PC axes\n",
    "    plt.plot(xs[i], ys[i], 'bo')\n",
    "    plt.text(xs[i]*1.2, ys[i]*1.2, list(read_tfidf_matrix.index)[i], color='b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pd.DataFrame(pca.components_, columns = read_tfidf_matrix.columns, index=[0, 1])\n",
    "components\n",
    "\n",
    "components.to_excel('E:/공모전/PCA Components.xlsx', sheet_name='sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
